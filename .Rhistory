sum(duplicated(combine[ , c("piteasting", "pitnorthing")]))
combine[duplicated(combine[ , c("piteasting", "pitnorthing")]),]
combine[combine$piteasting==265805.7&combine$pitnorthing==3950490,]
combine$pitnorthing==3950490
combine$pitnorthing
combine$pitnorthing[1]
class(combine$pitnorthing[1])
sum(combine$pitnorthing[1])
sum(combine$pitnorthing==3950490)
combine[ , c("piteasting", "pitnorthing")]
unique(combine)
dim(unique(combine))
dim(combine)
combine[ , c("piteasting", "pitnorthing")]
location=combine[ , c("piteasting", "pitnorthing")]
head(location)
class(location)
unique(location)
dim(unique(location))
dim(combine)
dim(unique(combine))
duplicated(location)
sum(duplicated(location))
dim(unique(location))
dim(combine)
location[duplicated(location),]
dup=location[duplicated(location),]
dup[1,1]
dup[1,1]==265805.7
combine[combine$piteasting==dup[1,1]&combine$pitnorthing==dup[1,2],]
combine[combine$piteasting==dup[2,1]&combine$pitnorthing==dup[2,2],]
combine[combine$piteasting==dup[3,1]&combine$pitnorthing==dup[3,2],]
combine[combine$piteasting==dup[4,1]&combine$pitnorthing==dup[4,2],]
combine <- bind_rows(temp_spc,temp_mgp,temp_sls) %>%
filter(!is.na(wtmean.estimatedOC.percent))
length(unique(combine$siteID))
dim(unique(combine))
dim(combine)
location=combine[ , c("piteasting", "pitnorthing")]
dim(unique(location))
sum(duplicated(location))
dup=location[duplicated(location),]
combine[combine$piteasting==dup[1,1]&combine$pitnorthing==dup[1,2],]
combine[combine$piteasting==dup[10,1]&combine$pitnorthing==dup[10,2],]
temp_mgp[temp_mgp$piteasting==dup[10,1]&temp_mgp$pitnorthing==dup[10,2],]
temp_sls[temp_sls$piteasting==dup[10,1]&temp_sls$pitnorthing==dup[10,2],]
temp_spc[temp_spc$piteasting==dup[10,1]&temp_spc$pitnorthing==dup[10,2],]
#start sls location and SOC :--------
#plot location: plot centroid
sls_spatial <- read.csv(paste0(dir, outputs, "/spatial/spatial_sls.csv"),
header = T, na.strings = "", stringsAsFactors = F)
#for organicCPercent:
sls_soilChem <- read.csv(paste0
(dir, portalFolder, "/filesToStack10086/stackedFiles/sls_soilChemistry.csv"),
header = T, na.strings = "", stringsAsFactors = F)
#for location of sampling
sls_CoreCollect <- read.csv(paste0
(dir, portalFolder, "/filesToStack10086/stackedFiles/sls_soilCoreCollection.csv"),
header = T, na.strings = "", stringsAsFactors = F)
sls.loc.file="/Users/zhuonanwang/Library/CloudStorage/OneDrive-EmoryUniversity/01_Projects-sihi/NEON-AOP/data/hs_lai/NEON_SOC_Locations/sls.loc.csv"
if(file.exists(sls.loc.file)) {
sls.loc <- read.csv(sls.loc.file)
}else{#download using getLocTOS
# calculate individual  locations
sls.loc <- getLocTOS(data=sls_CoreCollect,
dataProd="sls_soilCoreCollection",
token = NEON_TOKEN)
class(sls.loc)
head(sls.loc)
write_csv(sls.loc,"/Users/zhuonanwang/Library/CloudStorage/OneDrive-EmoryUniversity/01_Projects-sihi/NEON-AOP/data/hs_lai/NEON_SOC_Locations/sls.loc.csv")
}
dim(sls_soilChem)#3242   33
sls_soilChem_short <- sls_soilChem %>%
filter(!is.na(organicCPercent)) %>%  # 806 row organicCPercent is NA,remove them
select(namedLocation,domainID,siteID, plotID,sampleID,collectDate,
organicCPercent) %>%
group_by(namedLocation,domainID,siteID, plotID,sampleID,collectDate) %>%
summarise(OC_percent=mean(organicCPercent),
n=n()) %>% ungroup
sls_soilChem_short
sls_soilChem_short <- sls_soilChem %>%
filter(!is.na(organicCPercent)) %>%  # 806 row organicCPercent is NA,remove them
select(namedLocation,domainID,siteID, plotID,sampleID,
organicCPercent) %>%
group_by(namedLocation,domainID,siteID, plotID,sampleID) %>%
summarise(OC_percent=mean(organicCPercent),
collectDate=first(collectDate),
n=n()) %>% ungroup
sls_soilChem
View(sls_soilChem)
sls_soilChem_short <- sls_soilChem %>%
filter(!is.na(organicCPercent)) %>%  # 806 row organicCPercent is NA,remove them
select(namedLocation,domainID,siteID, plotID,sampleID,collectDate,
organicCPercent) %>%
group_by(namedLocation,domainID,siteID, plotID,sampleID) %>%
summarise(OC_percent=mean(organicCPercent),
collectDate=first(collectDate),
n=n()) %>% ungroup
dim(sls_soilChem_short) #2282    8
sls.loc_short <- sls.loc %>%
select(namedLocation,domainID,siteID, plotID,sampleID,
nlcdClass,
collectDate,sampleTiming,horizon,sampleTopDepth,sampleBottomDepth,
utmZone,adjNorthing,adjEasting)
sls30cm_locationSOC <- left_join(sls_soilChem_short,sls.loc_short,
by=c( "namedLocation","domainID","siteID","plotID",
"sampleID","collectDate")) %>%
filter(!is.na(OC_percent))   #2282   16
View(sls30cm_locationSOC)
sls30cm_locationSOC$collectDate <- substr(sls30cm_locationSOC$collectDate, 1, 10)
sls30cm_locationSOC$idcolumn <- sls30cm_locationSOC$sampleID
# remove O and M to make profile id
sls30cm_locationSOC <- sls30cm_locationSOC %>%
mutate(across('idcolumn', str_replace,  "-[OM]-", "-"))
View(sls30cm_locationSOC)
temp_sls[temp_sls$piteasting==dup[10,1]&temp_sls$pitnorthing==dup[10,2],]
#some data are same profile but collectDate wrong, not in same day.
#some data have same idcolumn but slightly different collectDate, correct them.:
sls30cmlocation_df <- sls30cm_locationSOC %>%
group_by(idcolumn) %>%
mutate(collectDate=first(collectDate)) %>%
ungroup #1910 unique(sls30cmlocation_df_test$idcolumn)
View(sls30cmlocation_df)
View(test=sls30cmlocation_df[sls30cmlocation_df$siteID=="GRSM",])
View(test)
(test=sls30cmlocation_df[sls30cmlocation_df$siteID=="GRSM",])
test2=test %>% group_by(idcolumn) %>% summarise(n=n())
View(test2)
temp_sls[temp_sls$piteasting==dup[10,1]&temp_sls$pitnorthing==dup[10,2],]
temp_sls[temp_sls$piteasting==dup[1,1]&temp_sls$pitnorthing==dup[1,2],]
dup=location[duplicated(location),]
combine[combine$piteasting==dup[10,1]&combine$pitnorthing==dup[10,2],]
location=combine[ , c("piteasting", "pitnorthing")]
dim(unique(location))
sum(duplicated(location))
dup=location[duplicated(location),]
dup
dim(unique(combine))
dim(combine)
#sls has some data are same location but different profile ID, some has only organic layer
location=combine[ , c("piteasting", "pitnorthing")]
dim(combine)
dim(unique(location))
dup=location[duplicated(location),]
dup
i=1
combine[combine$piteasting==dup[i,1]&combine$pitnorthing==dup[i,2],]
i=2
combine[combine$piteasting==dup[i,1]&combine$pitnorthing==dup[i,2],]
i=3
combine[combine$piteasting==dup[i,1]&combine$pitnorthing==dup[i,2],]
i=4
combine[combine$piteasting==dup[i,1]&combine$pitnorthing==dup[i,2],]
test=combine
class(combine[combine$piteasting==dup[i,1]&combine$pitnorthing==dup[i,2],]$collectDate)
a=as.Date("2015-08-27")
a
class(a)
b=as.Date("2015-09-03")
b>a
b
class(combine[combine$piteasting==dup[i,1]&combine$pitnorthing==dup[i,2],]$wtmean.estimatedOC.percent)
test2=test %>% group_by(domainID,siteID,utmZone,piteasting,pitnorthing) %>%
slice_max(wtmean.estimatedOC.percent)
dim(test2)
dim(test)
test2[test2$piteasting==dup[i,1]&test2$pitnorthing==dup[i,2],]
test[test$piteasting==dup[i,1]&test$pitnorthing==dup[i,2],]
combine <- bind_rows(temp_spc,temp_mgp,temp_sls) %>%
filter(!is.na(wtmean.estimatedOC.percent))
#sls has some data are same location but different profile ID, some has only organic layer
location=combine[ , c("piteasting", "pitnorthing")]
dim(combine)#2669    7
dim(unique(location))#2656    2 some data has same location
dup=location[duplicated(location),]
i=1
combine[combine$piteasting==dup[i,1]&combine$pitnorthing==dup[i,2],]
combine_clean=combine %>% group_by(domainID,siteID,utmZone,piteasting,pitnorthing) %>%
slice_max(wtmean.estimatedOC.percent)
dim(combine_clean)
dim(combine)
combine_clean[combine_clean$piteasting==dup[i,1]&combine_clean$pitnorthing==dup[i,2],]
combine[combine$piteasting==dup[i,1]&combine$pitnorthing==dup[i,2],]
write_csv(combine_clean,"/Users/zhuonanwang/Library/CloudStorage/OneDrive-EmoryUniversity/01_Projects-sihi/NEON-AOP/data/hs_lai/NEON_SOC_Locations/spc_mgp_sls_30cm_locationSOC.csv")
#FINAL products sls,spc,mgp 30cm soil SOC and location in UTM-----
write_csv(combine_clean,"/Users/zhuonanwang/Library/CloudStorage/OneDrive-EmoryUniversity/01_Projects-sihi/NEON-AOP/data/hs_lai/NEON_SOC_Locations/spc_mgp_sls_30cm_locationSOC.csv")
View(combine_clean)
#FINAL products sls,spc,mgp 30cm soil SOC and location in UTM-----
finalproduct <- "/Users/zhuonanwang/Library/CloudStorage/OneDrive-EmoryUniversity/01_Projects-sihi/NEON-AOP/data/hs_lai/NEON_SOC_Locations/spc_mgp_sls_30cm_locationSOC.csv"
#FINAL products sls,spc,mgp 30cm soil SOC and location in UTM-----
finalproduct <- "/Users/zhuonanwang/Library/CloudStorage/OneDrive-EmoryUniversity/01_Projects-sihi/NEON-AOP/data/hs_lai/NEON_SOC_Locations/spc_mgp_sls_30cm_locationSOC.csv"
file.exists(finalproduct)
slice()
?dice
spc_mgp_sls_30cm_locationSOC <- "/Users/zhuonanwang/Library/CloudStorage/OneDrive-EmoryUniversity/01_Projects-sihi/NEON-AOP/data/hs_lai/NEON_SOC_Locations/spc_mgp_sls_30cm_locationSOC.csv"
spc_mgp_sls_30cm_locationSOC <- read_csv("/Users/zhuonanwang/Library/CloudStorage/OneDrive-EmoryUniversity/01_Projects-sihi/NEON-AOP/data/hs_lai/NEON_SOC_Locations/spc_mgp_sls_30cm_locationSOC.csv")
spc_mgp_sls_30cm_locationSOC
View(spc_mgp_sls_30cm_locationSOC)
1:length(spc_mgp_sls_30cm_locationSOC)
1:nrow(spc_mgp_sls_30cm_locationSOC)
spc_mgp_sls_30cm_locationSOC$rowID=1:nrow(spc_mgp_sls_30cm_locationSOC)
View(spc_mgp_sls_30cm_locationSOC)
spc_mgp_sls_30cm_locationSOC
site="HARV"
# HARV_spc <- spc_mgp_sls_30cm_locationSOC <- read_csv("/Users/zhuonanwang/Library/CloudStorage/OneDrive-EmoryUniversity/01_Projects-sihi/NEON-AOP/data/hs_lai/NEON_SOC_Locations/spc_mgp_sls_30cm_locationSOC.csv")
# spc_mgp_sls_30cm_locationSOC$rowID=1:nrow(spc_mgp_sls_30cm_locationSOC)
HARV_spc <-  spc_mgp_sls_30cm_locationSOC %>% filter(siteID == site) %>%
arrange(rowID)
View(HARV_spc)
#We want the soil plot hyperspectral data, all plots have soil(spc) measurements
cat(HARV_spc$utmZone)
HARV_spc
View(HARV_spc)
HARV_spc$piteasting
HARV_spc$pitnorthing
HARV_spc$rowID
cat(HARV_spc$utmZone)
east <- HARV_spc$piteasting
names(east) <- HARV_spc$rowID
north <- HARV_spc$pitnorthing
names(north) <- HARV_spc$rowID
cat(paste0("east: ",east,"\n"))
cat(paste0("north: ",east,"\n"))
site=='BLAN'
site='BLAN'
# HARV_spc <- spc_mgp_sls_30cm_locationSOC <- read_csv("/Users/zhuonanwang/Library/CloudStorage/OneDrive-EmoryUniversity/01_Projects-sihi/NEON-AOP/data/hs_lai/NEON_SOC_Locations/spc_mgp_sls_30cm_locationSOC.csv")
# spc_mgp_sls_30cm_locationSOC$rowID=1:nrow(spc_mgp_sls_30cm_locationSOC)
HARV_spc <-  spc_mgp_sls_30cm_locationSOC %>% filter(siteID == site) %>%
arrange(rowID)
View(HARV_spc)
#View(HARV_spc)
#We want the soil plot hyperspectral data, all plots have soil(spc) measurements
cat(HARV_spc$utmZone)
east <- HARV_spc$piteasting
names(east) <- HARV_spc$rowID
north <- HARV_spc$pitnorthing
names(north) <- HARV_spc$rowID
cat(paste0("east: ",east,"\n"))
cat(paste0("north: ",east,"\n"))
which(east>250000)
#View(HARV_spc)
#We want the soil plot hyperspectral data, all plots have soil(spc) measurements
cat(HARV_spc$utmZone)
site="HARV"
# HARV_spc <- spc_mgp_sls_30cm_locationSOC <- read_csv("/Users/zhuonanwang/Library/CloudStorage/OneDrive-EmoryUniversity/01_Projects-sihi/NEON-AOP/data/hs_lai/NEON_SOC_Locations/spc_mgp_sls_30cm_locationSOC.csv")
# spc_mgp_sls_30cm_locationSOC$rowID=1:nrow(spc_mgp_sls_30cm_locationSOC)
HARV_spc <-  spc_mgp_sls_30cm_locationSOC %>% filter(siteID == site) %>%
arrange(rowID)
#View(HARV_spc)
#We want the soil plot hyperspectral data, all plots have soil(spc) measurements
cat(HARV_spc$utmZone)
east <- HARV_spc$piteasting
names(east) <- HARV_spc$rowID
north <- HARV_spc$pitnorthing
names(north) <- HARV_spc$rowID
cat(paste0("east: ",east,"\n"))
cat(paste0("north: ",east,"\n"))
#convert easting & northing coordinates for Blandy (BLAN)
# Blandy contains plots in 18N and plots in 17N; flight data are all in 17N
if(site=='BLAN' & length(which(east<=250000))>0) {
easting17 <- east[which(east>250000)]
northing17 <- north[which(east>250000)]
easting18 <- east[which(east<=250000)]
northing18 <- north[which(east<=250000)]
df18 <- cbind(easting18, northing18)
df18 <- data.frame(df18)
names(df18) <- c('easting','northing')
sp::coordinates(df18) <- c('easting', 'northing')
epsg.z <- 32617
if(utils::packageVersion("sp")<"1.4.2") {
sp::proj4string(df18) <- sp::CRS('+proj=utm +zone=18N ellps=WGS84')
df18conv <- sp::spTransform(df18, sp::CRS('+proj=utm +zone=17N ellps=WGS84'))
} else {
raster::crs(df18) <- sp::CRS("+proj=utm +zone=18")
df18conv <- sp::spTransform(df18, sp::CRS(paste("+init=epsg:", epsg.z, sep='')))
}
east <- c(easting17, df18conv$easting)
north <- c(northing17, df18conv$northing)
cat('Blandy (BLAN) plots include two UTM zones, flight data are all in 17N.
Coordinates in UTM zone 18N have been converted to 17N to download the correct tiles.
You will need to make the same conversion to connect airborne to ground data.')
}
print(paste0("sites location numbers: ",length(east)))
length(north)
#year, which year is the best, hwo to decide the year
# query the products endpoint for the product requested
source("/Users/zhuonanwang/Library/CloudStorage/OneDrive-EmoryUniversity/01_Projects-sihi/NEON-AOP/code/getAPI.R")
#year, which year is the best, hwo to decide the year
# query the products endpoint for the product requested
source("/Users/zhuonanwang/Library/CloudStorage/OneDrive-EmoryUniversity/01_Projects-sihi/NEON-AOP/code/My_functions/getAPI.R")
prod.req <- getAPI(apiURL = paste("http://data.neonscience.org/api/v0/products/", "DP3.30006.001", sep=""),
token = NEON_TOKEN)
avail <- jsonlite::fromJSON(httr::content(prod.req, as='text', encoding='UTF-8'),
simplifyDataFrame=TRUE, flatten=TRUE)
avail
# error message if product not found
if(!is.null(avail$error$status)) {
stop(paste("No data found for product","DP3.30006.001", sep=" "))
}
# get the urls for months with data available, and subset to site
month.urls <- unlist(avail$data$siteCodes$availableDataUrls)
month.urls
month.urls <- grep(site, month.urls, value = TRUE)
month.urls
str_split(month.urls, "/")
avilb_year <- str_split(month.urls, "/") %>%
map_vec(9) %>%
substr(1,4) %>%
as.numeric()
avilb_year
latest_year <- max(avilb_year)
cat(paste0("site:",site,"\navailable AOP data in year: "),avilb_year,
"\nlatest year: ",latest_year)
# get the urls for months with data available, and subset to site
month.urls <- unlist(avail$data$siteCodes$availableDataUrls)
month.urls <- grep(site, month.urls, value = TRUE)
month.urls
getwd()
getwd()
?map_vec
??map_vec
library(neonUtilities)
NEON_TOKEN <- "eyJ0eXAiOiJKV1QiLCJhbGciOiJFUzI1NiJ9.eyJhdWQiOiJodHRwczovL2RhdGEubmVvbnNjaWVuY2Uub3JnL2FwaS92MC8iLCJzdWIiOiJ6aHVvbmFuLndhbmdAZW1vcnkuZWR1Iiwic2NvcGUiOiJyYXRlOnB1YmxpYyIsImlzcyI6Imh0dHBzOi8vZGF0YS5uZW9uc2NpZW5jZS5vcmcvIiwiZXhwIjoxNzg3Njk4ODU4LCJpYXQiOjE2MzAwMTg4NTgsImVtYWlsIjoiemh1b25hbi53YW5nQGVtb3J5LmVkdSJ9.Ug3DuSLWLT0NsZy5pHKs8Wh51C9-tjxoCccbhrUW0m49n1GikPp0EQ3jZc3c6FFzRpDk04UXA-k4fECyxzqnmQ"
library(geoNEON)
library(neonhs)
library(rhdf5)
library(tidyverse)
library(terra)
library(raster)
library(rgdal)
library(sf)
# If file sizes are large, increase the timeout limit on your machine:
options(timeout=60*120)
getOption('timeout')
#----------------------------------------------------------------
#define fucntion download_HS_with_buffer()
#single site,
#year=0 then use latest available year download;
#year=2019 then use 2019 year download
download_HS_with_buffer <- function(wd ="/Users/zhuonanwang/Library/CloudStorage/OneDrive-EmoryUniversity/01_Projects-sihi/NEON-AOP/data/hs_lai/",
spc_mgp_sls_30cm_locationSOC,
site="HARV",
downloadAOPfile=FALSE,
year=2021,
whichpc="personal_pc") {
if(whichpc=="/Users/zwang61"){
user_wd="/Users/zwang61/Library/CloudStorage/OneDrive-EmoryUniversity/01_Projects-sihi/"
}else{
user_wd="/Users/zhuonanwang/Library/CloudStorage/OneDrive-EmoryUniversity/01_Projects-sihi/"
}
# we want to save our files. Be sure to move the download into your working directory!
setwd(wd)
cat("DP3.30006.001, hyperspectral data: \n")
HARV_spc <-  spc_mgp_sls_30cm_locationSOC %>% filter(siteID == site) %>%
arrange(rowID)
cat(site," site: \n")
#We want the soil plot hyperspectral data, all plots have soil(spc) measurements
# cat(HARV_spc$utmZone)
east <- HARV_spc$piteasting
names(east) <- HARV_spc$rowID
north <- HARV_spc$pitnorthing
names(north) <- HARV_spc$rowID
# cat(paste0("east: ",east,"\n"))
# cat(paste0("north: ",east,"\n"))
unique(HARV_spc$utmZone)
#year, which year is the best, hwo to decide the year
# query the products endpoint for the product requested
source(paste0(user_wd,"/NEON-AOP/code/My_functions/getAPI.R"))
prod.req <- getAPI(apiURL = paste("http://data.neonscience.org/api/v0/products/", "DP3.30006.001", sep=""),
token = NEON_TOKEN)
avail <- jsonlite::fromJSON(httr::content(prod.req, as='text', encoding='UTF-8'),
simplifyDataFrame=TRUE, flatten=TRUE)
# error message if product not found
if(!is.null(avail$error$status)) {
stop(paste("No data found for product","DP3.30006.001", sep=" "))
}
# get the urls for months with data available, and subset to site
month.urls <- unlist(avail$data$siteCodes$availableDataUrls)
month.urls <- grep(site, month.urls, value = TRUE)
avilb_year <- str_split(month.urls, "/") %>%
map_vec(9) %>%
substr(1,4) %>%
as.numeric()
latest_year <- max(avilb_year)
cat("\n",paste0("site:",site,"\navailable AOP data in year: "),avilb_year,
"\nlatest year: ",latest_year,"\n")
if(year==0){
cat("didn't specify year, use  available latest_year","\n")
}else{
if(!year %in% avilb_year){
cat("user choose year is not in available year at this site.----->\n",
"use  available latest_year",latest_year,"\n\n")
}else{
latest_year <- year
cat("download user choose year, latest_year=", year,"\n")
}
}
# Download hyperspectral imagery from NEON-AOP-----------------------------
# Now we have the UTM coordinates from the plot HARV_001 we can download
# the Hyperspectral Remote Sensing Data in HDF5 Format for that site.
#make sure there is no this sites HDF5 files
dir <- paste0(wd,"/DP3.30006.001/neon-aop-products/",latest_year,"/FullSite/",unique(HARV_spc$domainID))
dirs <- list.dirs(dir)
sites_dirs <- grep(paste0(latest_year,"_",site), dirs)
length(sites_dirs)#if there is no this sites file, length==0,then download them.
if (length(sites_dirs)==0){
cat("\n","this site HDF5 data doesn't exists\n")
}else{
cat("\n","this site HDF5 data exists\n")
}
cat("Parameter downloadAOPfile =", downloadAOPfile,"\n")
if(downloadAOPfile==TRUE){
#warning during download:
# If file sizes are large, increase the timeout limit on your machine:
options(timeout=60*120)
getOption('timeout')
cat("download this site plots located HDF5 Tiles \n")
#set  check.size = F, so continously download
#HS download site-year-plot by tile
byTileAOP(dpID = "DP3.30006.001", # NEON-AOP product
site = site, # Site code
year = latest_year, # Year
check.size = F,
easting = east, northing = north, # Coordinates UTM
buffer=20,#with buffer could include more tiles
savepath = paste0(user_wd,"/NEON-AOP/data/hs_lai/"), # Path
token = NEON_TOKEN)
}else{
cat("\n","do not allow to download this site HDF5 data\n")
}
}
#____________________________
#start to download each site----
#____________________________
{
spc_mgp_sls_30cm_locationSOC <- read_csv(paste0(user_wd,"NEON-AOP/data/hs_lai/NEON_SOC_Locations/spc_mgp_sls_30cm_locationSOC.csv"))
spc_mgp_sls_30cm_locationSOC$rowID=1:nrow(spc_mgp_sls_30cm_locationSOC)
allsites=unique(spc_mgp_sls_30cm_locationSOC$siteID)
length(allsites) #47 we have all NEON has 47 terrestrial sites
allsites
}
/lustre/or-scratch/cades-ccsi/scratch/z2w/NEON_hs/data/
user_wd="/lustre/or-scratch/cades-ccsi/scratch/z2w/NEON_hs/"
spc_mgp_sls_30cm_locationSOC <- read_csv(paste0(user_wd,"spc_mgp_sls_30cm_locationSOC.csv"))
paste0(user_wd,"spc_mgp_sls_30cm_locationSOC.csv")
?map_vec
??map_vec
wd
wd="/lustre/or-scratch/cades-ccsi/scratch/z2w/NEON_hs/data/"
paste0(wd,"/DP3.30006.001/neon-aop-products/",latest_year,"/FullSite/")
latest_year=2021
paste0(wd,"/DP3.30006.001/neon-aop-products/",latest_year,"/FullSite/")
#make sure there is no this sites HDF5 files
dir <- paste0(wd,"/DP3.30006.001/neon-aop-products/",latest_year,"/FullSite/")
dir
month.urls
# look at the HDF5 file structure
View(h5ls(f,all=T))
# Define the file name to be opened
f <- paste0(wd,"NEON_hyperspectral_tutorial_example_subset.h5")
wd <- "/Users/zhuonanwang/Library/CloudStorage/OneDrive-EmoryUniversity/01_Projects-sihi/NEON-AOP/code/R_learning/" #This will depend on your local environment
# Define the file name to be opened
f <- paste0(wd,"NEON_hyperspectral_tutorial_example_subset.h5")
# look at the HDF5 file structure
View(h5ls(f,all=T))
f
library(rhdf5)
# look at the HDF5 file structure
View(h5ls(f,all=T))
h5ls(f)
h5ls(f) %>% class()
library(tidyverse)
h5ls(f) %>% class()
h5ls(f)
h5ls(f)$name
h5ls(f)$name[1]
f
flightsite="SJER"
path <- paste0("/",f,flightsite, '/Reflectance/Reflectance_Data')
path
reflInfo <- h5readAttributes(path)
paste0("/",f,flightsite, '/Reflectance/Reflectance_Data')
path <- paste0(f,"/",flightsite, '/Reflectance/Reflectance_Data')
path
reflInfo <- h5readAttributes(path)
f
path <- paste0("/",flightsite, '/Reflectance/Reflectance_Data')
path
datapath <- paste0("/",flightsite, '/Reflectance/Reflectance_Data')
reflInfo <- h5readAttributes(f,datapath)
reflInfo
f
datapath
reflInfo$Cloud_conditions
f
flightnames <- basename(f)
flightnames
c(flightnames,reflInfo$Cloud_conditions)
c(names=flightnames,cloud=reflInfo$Cloud_conditions)
data.frame(names=flightnames,cloud=reflInfo$Cloud_conditions)
data.frame(names=flightnames,cloud=reflInfo$Cloud_conditions) %>% dim()
oneflightline <- data.frame(names=flightnames,cloud=reflInfo$Cloud_conditions)
oneflightline
out <- NULL
oneflightline
out=rbind(out,oneflightline)
out
out=rbind(out,oneflightline)
out
View(out)
paste0(wd,"DP1.30006.001/cloud/",
site,"_",latest_year,".csv")
